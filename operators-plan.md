# The Grand Grimoire of Operators

A comprehensive exploration across array languages, functional programming, category theory, signal processing, and beyond.

**Sources**: APL, J, K/Q, BQN, Haskell, Julia, NumPy/PyTorch, R, Mathematica, MATLAB, Forth, Lisp, Category Theory, Signal Processing, Statistics, Linear Algebra, and pure mathematical notation.

---

## 1. Higher-Order Function Combinators

### Scan (Prefix Reduction) â€” `\` or `â€`
**APL**: `+\` gives running sums. Essential for cumulative operations.
```fluent
(â€): TensorScan,        ; or (\)
+â€ [1,2,3,4]            ; â†’ [1, 3, 6, 10]
*â€ [1,2,3,4]            ; â†’ [1, 2, 6, 24] (running product)
```
**Implementation**: `tf.cumsum`, `tf.cumprod`, or generic scan with any binary op.

### Outer Product â€” `âˆ˜.` or `âŠ—`
**APL**: `âˆ˜.Ã—` creates multiplication table. Fundamental for combinatorics.
```fluent
(âŠ—): TensorOuter,
[1,2,3] âŠ—(*) [1,2,3]    ; â†’ [[1,2,3],[2,4,6],[3,6,9]]
[1,2,3] âŠ—(-) [1,2,3]    ; â†’ [[0,-1,-2],[1,0,-1],[2,1,0]]
```
**Implementation**: Reshape + broadcast multiplication.

### Inner Product â€” `f.g`
**APL**: `+.Ã—` is matrix multiply. Generalized: any two ops.
```fluent
A +.Ã— B                  ; standard matmul
A âˆ¨.âˆ§ B                  ; boolean matrix "or-and" product
A max.+ B                ; tropical semiring (shortest path)
```
**Implementation**: Already have `matmul`, but generalized version is powerful.

### Fork â€” `(f g h)`
**BQN/J**: Apply three functions, combine results: `(f x) g (h x)`
```fluent
fork: { f, g, h | { x | g(f(x), h(x)) } },
avg: fork(sum, Ã·, #),    ; sum(x) / length(x)
avg([1,2,3,4,5])         ; â†’ 3
```

### Hook â€” `(f g)`
**J**: `x f (g x)` â€” apply g first, then f with original
```fluent
hook: { f, g | { x | f(x, g(x)) } },
demean: hook(-, Î¼),      ; x - mean(x)
demean([1,2,3,4,5])      ; â†’ [-2,-1,0,1,2]
```

### Compose â€” `âˆ˜`
```fluent
(âˆ˜): { f, g | { x | f(g(x)) } },
sincos: sin âˆ˜ cos,
```

### Flip/Commute â€” `â¨` or `Ëœ`
**APL**: Swap arguments or duplicate single argument
```fluent
(â¨): { f | { x, y | f(y, x) } },
3 -â¨ 10                  ; â†’ 7 (10 - 3, not 3 - 10)
-â¨ 5                     ; â†’ 0 (5 - 5, self-application)
```

---

## 2. Array Manipulation

### Rotate â€” `âŒ½` (reverse) and `âŠ–` (rotate by n)
```fluent
(âŒ½): TensorReverse,      ; already have this
(âŠ–): TensorRotate,       ; rotate by n positions
2 âŠ– [1,2,3,4,5]          ; â†’ [3,4,5,1,2]
```
**Implementation**: `tf.concat(tf.slice(...), tf.slice(...))`

### Take/Drop â€” `â†‘` / `â†“`
**APL**: First/last n elements, negative for end
```fluent
(â†‘): TensorTake,
(â†“): TensorDrop,
3 â†‘ [1,2,3,4,5]          ; â†’ [1,2,3]
-2 â†‘ [1,2,3,4,5]         ; â†’ [4,5]
2 â†“ [1,2,3,4,5]          ; â†’ [3,4,5]
```

### Replicate/Compress â€” `/` (dyadic)
**APL**: Filter by boolean mask or repeat by counts
```fluent
[1,0,1,0,1] / [10,20,30,40,50]  ; â†’ [10,30,50]
[2,0,3] / [1,2,3]               ; â†’ [1,1,3,3,3]
```
Note: Already have `mask`, but replicate is more general.

### Unique/Nub â€” `âˆª`
```fluent
(âˆª): TensorUnique,
âˆª [1,2,2,3,3,3]          ; â†’ [1,2,3]
```
**Implementation**: `tf.unique`

### Membership/Element of â€” `âˆˆ`
```fluent
(âˆˆ): TensorMembership,
[2,5] âˆˆ [1,2,3,4]        ; â†’ [1,0] (2 is in, 5 is not)
```

### Index of â€” `â³` (dyadic)
**APL**: Find positions of elements
```fluent
(â³): FunctionCascade((TensorIndexOf, TensorIota)),
â³ 5                      ; â†’ [0,1,2,3,4] (iota, same as 0::5)
[1,2,3] â³ [2,4]          ; â†’ [1, 3] (index of 2 is 1, 4 not found â†’ length)
```

### Grade â€” `â‹` (up) and `â’` (down)
**APL**: Return indices that would sort the array
```fluent
(â‹): TensorGradeUp,
(â’): TensorGradeDown,
â‹ [30,10,20]             ; â†’ [1,2,0] (indices to sort ascending)
â’ [30,10,20]             ; â†’ [0,2,1] (indices to sort descending)
```
**Implementation**: `tf.topk` or argsort equivalent

### Enclose/Disclose â€” `âŠ‚` / `âŠƒ`
**APL**: Box/unbox for nested arrays
```fluent
(âŠ‚): TensorEnclose,      ; wrap as single element
(âŠƒ): TensorDisclose,     ; unwrap / first element
âŠƒ [[1,2],[3,4]]          ; â†’ [1,2]
```

---

## 3. Mathematical Operations

### Factorial/Binomial â€” `!`
```fluent
(!): FunctionCascade((TensorBinomial, TensorFactorial)),
! 5                      ; â†’ 120
3 ! 5                    ; â†’ 10 (5 choose 3)
```
**Implementation**: `tf.exp(tf.lgamma(n+1))` for factorial

### GCD/LCM â€” `âˆ¨` / `âˆ§`
**APL**: Greatest common divisor, least common multiple
```fluent
(âˆ¨): TensorGCD,
(âˆ§): TensorLCM,
12 âˆ¨ 18                  ; â†’ 6
12 âˆ§ 18                  ; â†’ 36
```

### Complex Numbers â€” `â„‘`, `â„œ`, `âˆ `
```fluent
(â„‘): TensorImaginary,
(â„œ): TensorReal,
(âˆ ): TensorAngle,        ; or TensorPhase
complex: { r, i | r + (i Ã— 1j) },
```

### Logarithm Base â€” `âŸ`
**APL**: `x âŸ y` is log base x of y
```fluent
(âŸ): TensorLogBase,
2 âŸ 8                    ; â†’ 3
10 âŸ 1000                ; â†’ 3
```

### Clamp/Clip
```fluent
clamp: { lo, hi, x | lo âŒˆ (x âŒŠ hi) },
; or as operator
(âŠ): TensorClamp,
[0, 1] âŠ [-0.5, 0.5, 1.5]  ; â†’ [0, 0.5, 1]
```

### Softmax â€” `Ïƒ`
```fluent
(Ïƒ): TensorSoftmax,
Ïƒ [1, 2, 3]              ; â†’ [0.09, 0.24, 0.67]
```
**Implementation**: `tf.softmax`

### Norm â€” `â€–`
```fluent
(â€–): TensorNorm,
â€– [3, 4]                 ; â†’ 5 (L2 norm)
2 â€– [3, 4]               ; â†’ 5 (explicit L2)
1 â€– [3, 4]               ; â†’ 7 (L1 norm)
```

---

## 4. Structural Operations

### Ravel/Flatten â€” `,`
**APL**: Flatten to 1D
```fluent
(,): TensorFlatten,
, [[1,2],[3,4]]          ; â†’ [1,2,3,4]
```
**Implementation**: `tf.reshape(x, [-1])`

### Split â€” `âŠ†`
```fluent
(âŠ†): TensorSplit,
3 âŠ† [1,2,3,4,5,6,7,8,9]  ; â†’ [[1,2,3],[4,5,6],[7,8,9]]
```
**Implementation**: `tf.split`

### Pad
```fluent
pad: TensorPad,
[[1,1], [2,2]] pad [[1,2],[3,4]]  ; pad with zeros
```
**Implementation**: `tf.pad`

### Diagonal â€” `âŒ¹` (monadic: diagonal, dyadic: solve)
**APL**: Extract diagonal or solve linear system
```fluent
(âŒ¹): FunctionCascade((TensorSolve, TensorDiagonal)),
âŒ¹ [[1,2],[3,4]]          ; â†’ [1,4] (diagonal)
A âŒ¹ b                    ; â†’ x where Ax = b
```

### Broadcast/Expand
```fluent
(â¤¢): TensorBroadcast,
[3, 1] â¤¢ [1, 2, 3]       ; â†’ [[1,2,3],[1,2,3],[1,2,3]]
```

---

## 5. Logic & Boolean

### All/Any â€” `âˆ€` / `âˆƒ`
```fluent
(âˆ€): TensorAll,
(âˆƒ): TensorAny,
âˆ€ [1, 1, 1]              ; â†’ 1 (all true)
âˆƒ [0, 0, 1]              ; â†’ 1 (any true)
```
**Implementation**: `tf.all`, `tf.any`

### Not â€” `Â¬`
```fluent
(Â¬): TensorNot,
Â¬ [0, 1, 0]              ; â†’ [1, 0, 1]
```
**Implementation**: `tf.logicalNot`

### And/Or/Xor â€” `âˆ§` / `âˆ¨` / `âŠ»`
Could overload `âˆ§`/`âˆ¨` for booleans vs GCD/LCM for integers.
```fluent
(âŠ»): TensorXor,
[1,0,1] âŠ» [1,1,0]        ; â†’ [0,1,1]
```

---

## 6. Special & Fun

### Stencil/Convolution â€” `âŒº`
**APL2**: Apply function to sliding windows
```fluent
(âŒº): TensorStencil,
3 âŒº(Î¼) [1,2,3,4,5]       ; â†’ [1.5, 2, 3, 4, 4.5] (moving average)
```
**Implementation**: Convolution or manual windowing

### Life/Cellular Automata helper
```fluent
neighbors: { grid |
  ; count of 8-neighbors for each cell
  ...
},
```

### Random Choice â€” `?`
**APL**: `?n` gives random from 0 to n-1, `m?n` gives m unique randoms
```fluent
(?): FunctionCascade((TensorDeal, TensorRoll)),
? 6                      ; â†’ random 0-5
3 ? 10                   ; â†’ 3 unique randoms from 0-9
```

### Interval â€” `â¸¤` `â¸¥` or `[]`
```fluent
interval: { lo, hi | { x | (x â‰¥ lo) * (x â‰¤ hi) } },
[0, 1] interval 0.5      ; â†’ 1 (in range)
```

---

## Priority Recommendations

### Tier 1 (High Value, Easy to Implement)
1. **`â€` Scan** â€” cumsum/cumprod are common, tf.js has them
2. **`â†‘`/`â†“` Take/Drop** â€” fundamental, easy with slice
3. **`â‹`/`â’` Grade** â€” argsort is essential for many algorithms
4. **`âˆª` Unique** â€” tf.unique exists
5. **`âˆ€`/`âˆƒ` All/Any** â€” tf.all/tf.any exist
6. **`,` Ravel/Flatten** â€” simple reshape

### Tier 2 (High Value, Moderate Effort)
1. **`âŠ—` Outer Product** â€” extremely powerful for combinatorics
2. **`â¨` Flip** â€” elegant, can be pure Fluent
3. **Fork/Hook** â€” define in prelude, no new primitives needed
4. **`âŠ–` Rotate** â€” useful for cyclic operations
5. **`!` Factorial/Binomial** â€” mathematical completeness

### Tier 3 (Nice to Have)
1. **`âŒº` Stencil** â€” powerful but complex
2. **`âŒ¹` Solve/Diagonal** â€” linear algebra completeness
3. **`âˆˆ` Membership** â€” set operations
4. **`â³` Index Of** â€” searching

---

## Implementation Notes

Many can be defined in PRELUDE without new TypeScript:
```fluent
; Combinators (pure Fluent)
flip: { f | { x, y | f(y, x) } },
fork: { f, g, h | { x | g(f(x), h(x)) } },
hook: { f, g | { x | f(x, g(x)) } },
compose: { f, g | { x | f(g(x)) } },
(â¨): flip,
(âˆ˜): compose,

; Using existing ops
flatten: { x | x â´ [-1] },
(,): flatten,
```

For tf.js-backed ones, need new TypeScript functions:
- `TensorCumSum`, `TensorCumProd` (tf.cumsum, tf.cumprod)
- `TensorUnique` (tf.unique)
- `TensorArgSort` / `TensorGradeUp` (tf.topk based)
- `TensorAll`, `TensorAny` (tf.all, tf.any)
- `TensorSplit` (tf.split)

---

## 7. J Language â€” Tacit Programming Mastery

### Gerunds â€” `\`` (verb trains)
**J**: Package verbs as data for later application
```fluent
gerund: List,            ; verbs as list
(+, -, Ã—) @ 0            ; apply first verb
```

### Agenda â€” `@.`
**J**: Conditional verb selection
```fluent
(@.): { conds, funcs, x | funcs_(conds(x))(x) },
(neg, +1, abs) @. sign   ; neg if negative, +1 if zero, abs if positive
```

### Bond/Curry â€” `&`
**J**: Partial application
```fluent
(&): { f, a | { x | f(a, x) } },
double: 2 & Ã—,
double(5)                ; â†’ 10
```

### Rank â€” `"`
**J**: Control rank of operation (apply at specific dimensions)
```fluent
("): TensorRank,
sum " 1                  ; sum along axis 1
f " [0, 1]               ; apply f at different ranks for each arg
```

### Under â€” `&.`
**J**: Conjugation â€” apply f, transform, apply g, untransform
```fluent
(&.): { f, g | { x | gâ»Â¹(f(g(x))) } },
round &. (Ã—100)          ; round to 2 decimal places
sort &. âŠ‚               ; sort boxed arrays
```

### Obverse â€” `:.`
**J**: Define inverse of a function
```fluent
(:.): { f, f_inv | ... },
encode :. decode,
```

### Power â€” `^:`
**J**: Apply function n times (already have âŸ³, but conditional version)
```fluent
(^:): { f, n | { x | f âŸ³ n } },
f ^: _ x                 ; apply until convergence
f ^: (cond) x            ; apply while condition holds
```

---

## 8. K/Q Language â€” Extreme Terseness

### Each-Left/Each-Right â€” `\:` / `/:`
**K**: Map with one fixed argument
```fluent
(\:): { f | { x, ys | ys ListMap { y | f(x, y) } } },
(/:): { f | { xs, y | xs ListMap { x | f(x, y) } } },
10 -\: [1,2,3]           ; â†’ [9, 8, 7]
[1,2,3] -/: 10           ; â†’ [-9, -8, -7]
```

### Over/Scan â€” `/` `\` as adverbs
**K**: Reduce and scan with initial value option
```fluent
+/ [1,2,3,4]             ; â†’ 10 (reduce)
+\ [1,2,3,4]             ; â†’ [1,3,6,10] (scan)
0 +/ [1,2,3]             ; â†’ 6 (with initial)
```

### Converge â€” `/` (monadic)
**K**: Apply until fixed point
```fluent
converge: { f, x | ... until f(x) = x },
{x % 2}/[1000]           ; â†’ 0 (halve until 0)
```

### Windows â€” `'`
**K**: Sliding windows
```fluent
('): TensorWindows,
3 ' [1,2,3,4,5]          ; â†’ [[1,2,3],[2,3,4],[3,4,5]]
```

### Prior â€” `':`
**K**: Apply to each pair (current, previous)
```fluent
(':): TensorPrior,
-': [1,3,6,10]           ; â†’ [1,2,3,4] (differences)
```

### Cross â€” `,\:`
**K**: Cartesian product
```fluent
cross: { a, b | a ,\: b },
[1,2] cross ["a","b"]    ; â†’ [[1,"a"],[1,"b"],[2,"a"],[2,"b"]]
```

---

## 9. BQN â€” Modern Array Language

### Before/After â€” `âŠ¸` / `âŸœ`
**BQN**: Compose with argument binding
```fluent
(âŠ¸): { f, g | { x | f(g(x), x) } },  ; (g x) f x
(âŸœ): { f, g | { x | f(x, g(x)) } },  ; x f (g x)
-âŠ¸Ã·                      ; (neg x) / x = -1
Ã·âŸœ2                      ; x / 2
```

### Atop/Over â€” `âˆ˜` / `â—‹`
**BQN**: Function composition variants
```fluent
(âˆ˜): { f, g | { x, y | f(g(x, y)) } },     ; f(g(x,y))
(â—‹): { f, g | { x, y | f(g(x), g(y)) } },  ; f(g(x), g(y))
+â—‹abs                    ; abs(x) + abs(y)
```

### Repeat â€” `âŸ`
**BQN**: Apply n times or until condition
```fluent
(âŸ): { f, n | f âŸ³ n },
double âŸ 3               ; double 3 times
```

### Cells â€” `Ë˜`
**BQN**: Apply to major cells (leading axis)
```fluent
(Ë˜): TensorCells,
reverseË˜ [[1,2],[3,4]]   ; reverse each row
```

### Each â€” `Â¨`
**BQN**: Map (explicit)
```fluent
(Â¨): ListMap,
+1Â¨ [1,2,3]              ; â†’ [2,3,4]
```

### Table â€” `âŒœ`
**BQN**: Outer product (all combinations)
```fluent
(âŒœ): TensorTable,
Ã—âŒœ [1,2,3]               ; multiplication table
```

### Fold â€” `Â´`
**BQN**: Explicit reduce
```fluent
(Â´): ListReduce,
+Â´ [1,2,3,4]             ; â†’ 10
```

### Insert â€” `Ë`
**BQN**: Reduce along first axis
```fluent
(Ë): TensorReduceFirst,
+Ë [[1,2],[3,4]]         ; â†’ [4,6]
```

### Group â€” `âŠ”`
**BQN**: Group elements by key
```fluent
(âŠ”): TensorGroup,
[0,1,0,1] âŠ” [1,2,3,4]    ; â†’ [[1,3],[2,4]]
```

### Classify â€” `âŠ` / `âŠ’`
**BQN**: Index of / progressive index of
```fluent
(âŠ): TensorClassify,
"abcabc" âŠ "abc"         ; â†’ [0,1,2,0,1,2]
```

### Occurrence Count â€” `âŠ’`
**BQN**: Count of each element so far
```fluent
(âŠ’): TensorOccurrence,
âŠ’ "abcabc"               ; â†’ [0,0,0,1,1,1]
```

### Mark Firsts â€” `âŠ‘`
**BQN**: Boolean mask of first occurrences
```fluent
(âŠ‘): TensorMarkFirsts,
âŠ‘ [1,2,1,3,2]            ; â†’ [1,1,0,1,0]
```

---

## 10. Haskell â€” Functional Purity

### Functor Map â€” `<$>` or `fmap`
```fluent
(<$>): { f, x | f(x) },  ; just map in our context
```

### Applicative â€” `<*>`
```fluent
(<*>): { fs, xs | ... }, ; apply list of functions to list of values
[(+1), (*2)] <*> [1,2]   ; â†’ [2,3,2,4]
```

### Bind/FlatMap â€” `>>=`
```fluent
(>>=): { xs, f | flatten(xs ListMap f) },
[1,2] >>= { x | [x, x*2] }  ; â†’ [1,2,2,4]
```

### Kleisli Composition â€” `>=>`
```fluent
(>=>): { f, g | { x | f(x) >>= g } },
```

### Arrow Combinators â€” `&&&`, `***`, `|||`
```fluent
(&&&): { f, g | { x | (f(x), g(x)) } },      ; fanout
(***): { f, g | { (x,y) | (f(x), g(y)) } },  ; split
(|||): { f, g | { e | either(f, g, e) } },   ; fanin
```

### Fix Point â€” `fix`
**Haskell**: Y combinator, self-referential definitions
```fluent
fix: { f | f(fix(f)) },
factorial: fix({ f, n | n â‰¤ 1 ? 1 : n Ã— f(n-1) }),
```

### Memoize
```fluent
memo: { f | ... cached version ... },
fib: memo({ f, n | n â‰¤ 1 ? n : f(n-1) + f(n-2) }),
```

### Zip/ZipWith
```fluent
zip: { a, b | ... },
zipWith: { f, a, b | ... },
[1,2,3] zipWith(+) [4,5,6]  ; â†’ [5,7,9]
```

### Unzip
```fluent
unzip: { pairs | (pairs ListMap first, pairs ListMap second) },
```

---

## 11. Category Theory Abstractions

### Identity â€” `id` or `ğŸ™`
```fluent
id: { x | x },
(ğŸ™): id,
```

### Constant â€” `const` or `K`
```fluent
const: { a | { _ | a } },
K: const,
K(5)(anything)           ; â†’ 5
```

### Flip/C combinator
```fluent
C: { f | { x, y | f(y, x) } },
```

### Compose/B combinator
```fluent
B: { f, g | { x | f(g(x)) } },
```

### Substitution/S combinator
```fluent
S: { f, g | { x | f(x)(g(x)) } },
```

### Duplicator/W combinator
```fluent
W: { f | { x | f(x)(x) } },
```

### Blackbird/B1 combinator
```fluent
B1: { f, g, h | { x | f(g(h(x))) } },
```

### Phoenix/Î¦ combinator
```fluent
Î¦: { f, g, h | { x | f(g(x), h(x)) } },  ; same as fork!
```

### Psi/on combinator
```fluent
on: { f, g | { x, y | f(g(x), g(y)) } },
compare on abs,          ; compare by absolute value
```

### Bifunctor â€” `bimap`
```fluent
bimap: { f, g, (a, b) | (f(a), g(b)) },
```

### Profunctor â€” `dimap`
```fluent
dimap: { f, g, h | { x | g(h(f(x))) } },
```

---

## 12. Signal Processing

### FFT/IFFT â€” `â„±` / `â„±â»Â¹`
```fluent
(â„±): TensorFFT,
(â„±â»Â¹): TensorIFFT,
â„± [1,0,1,0]              ; frequency domain
â„±â»Â¹(â„±(x))               ; â†’ x (round-trip)
```

### Convolution â€” `âŠ›` or `âˆ—`
```fluent
(âŠ›): TensorConvolve,
signal âŠ› kernel,
[1,2,3] âŠ› [0.5, 0.5]     ; moving average
```

### Correlation â€” `â‹†`
```fluent
(â‹†): TensorCorrelate,
signal â‹† template,       ; cross-correlation
signal â‹† signal,         ; auto-correlation
```

### Differentiate/Integrate (discrete)
```fluent
(âˆ‚): TensorDiff,         ; differences
(âˆ«): TensorCumSum,       ; cumulative sum (discrete integral)
âˆ‚ [1,3,6,10]             ; â†’ [2,3,4]
âˆ« [2,3,4]                ; â†’ [2,5,9]
```

### Interpolate
```fluent
lerp: { t, a, b | a + t Ã— (b - a) },
interp: { xs, ys, x | ... },  ; interpolate
```

### Resample
```fluent
resample: { x, n | ... },
upsample: { x, n | ... },
downsample: { x, n | ... },
```

### Filter (IIR/FIR)
```fluent
fir: { coeffs, x | x âŠ› coeffs },
iir: { a, b, x | ... },
```

### Window Functions
```fluent
hann: { n | 0.5 Ã— (1 - cos(2Ï€ Ã— (0::n) / n)) },
hamming: { n | 0.54 - 0.46 Ã— cos(2Ï€ Ã— (0::n) / n) },
blackman: { n | ... },
kaiser: { n, Î² | ... },
```

---

## 13. Statistics & Probability

### Variance/StdDev â€” `ÏƒÂ²` / `Ïƒ`
```fluent
(ÏƒÂ²): TensorVariance,
(Ïƒ): TensorStdDev,
Ïƒ [1,2,3,4,5]            ; â†’ 1.414...
```

### Covariance/Correlation Matrix
```fluent
cov: TensorCovariance,
corr: TensorCorrelation,
```

### Percentile/Quantile â€” `â„˜`
```fluent
(â„˜): TensorQuantile,
0.5 â„˜ [1,2,3,4,5]        ; â†’ 3 (median)
[0.25, 0.5, 0.75] â„˜ x    ; quartiles
```

### Histogram
```fluent
hist: { x, bins | ... },
```

### Random Distributions
```fluent
uniform: { lo, hi, shape | ... },
normal: { Î¼, Ïƒ, shape | ... },
poisson: { Î», shape | ... },
bernoulli: { p, shape | ... },
categorical: { probs | ... },
```

### Sampling
```fluent
sample: { x, n | ... },          ; sample n from x
sampleWith: { x, n | ... },      ; with replacement
shuffle: { x | ... },
```

### Moments
```fluent
moment: { x, k | Î¼(x^k) },       ; k-th moment
skewness: { x | ... },
kurtosis: { x | ... },
```

### Z-score / Standardize
```fluent
zscore: { x | (x - Î¼(x)) / Ïƒ(x) },
standardize: zscore,
```

### Moving Statistics
```fluent
movingAvg: { x, n | ... },
movingStd: { x, n | ... },
ewma: { x, Î± | ... },            ; exponentially weighted moving average
```

---

## 14. Linear Algebra Deep Dive

### Decompositions
```fluent
svd: TensorSVD,                  ; singular value decomposition
qr: TensorQR,                    ; QR decomposition
lu: TensorLU,                    ; LU decomposition
cholesky: TensorCholesky,        ; Cholesky decomposition
eig: TensorEigen,                ; eigenvalues/vectors
```

### Matrix Properties
```fluent
det: TensorDeterminant,
tr: TensorTrace,                 ; trace (sum of diagonal)
rank: TensorRank,
cond: TensorConditionNumber,
```

### Matrix Operations
```fluent
inv: TensorInverse,
pinv: TensorPseudoInverse,       ; Moore-Penrose
(â€ ): TensorConjugateTranspose,   ; Hermitian transpose
```

### Norms
```fluent
norm: { x, p | ... },
frobenius: { x | norm(x, "fro") },
nuclear: { x | Î£(svd(x)_1) },    ; nuclear norm
spectral: { x | max(svd(x)_1) },
```

### Special Matrices
```fluent
zeros: { shape | fill(shape, 0) },
ones: { shape | fill(shape, 1) },
diag: { v | ... },               ; diagonal matrix from vector
triu: TensorUpperTriangular,
tril: TensorLowerTriangular,
toeplitz: { c, r | ... },
hankel: { c, r | ... },
circulant: { c | ... },
vandermonde: { x, n | ... },
```

### Kronecker/Hadamard Products
```fluent
(âŠ—): TensorKronecker,            ; Kronecker product
(âŠ™): TensorHadamard,             ; element-wise (already have Ã—)
```

### Solve Systems
```fluent
solve: { A, b | A âŒ¹ b },
lstsq: { A, b | ... },           ; least squares
```

---

## 15. Set Operations

### Union/Intersection/Difference
```fluent
(âˆª): TensorUnion,
(âˆ©): TensorIntersection,
(âˆ–): TensorSetDifference,
(â–³): TensorSymmetricDifference,
```

### Subset/Superset
```fluent
(âŠ‚): TensorSubset,               ; proper subset
(âŠ†): TensorSubsetEq,
(âŠƒ): TensorSuperset,
(âŠ‡): TensorSupersetEq,
```

### Power Set
```fluent
(â„˜): TensorPowerSet,
â„˜ [1,2,3]                        ; â†’ [[], [1], [2], [3], [1,2], [1,3], [2,3], [1,2,3]]
```

---

## 16. String/Sequence Operations (if extended beyond tensors)

### Match/Find
```fluent
match: { pattern, text | ... },
findAll: { pattern, text | ... },
```

### Split/Join
```fluent
split: { sep, s | ... },
join: { sep, xs | ... },
```

### Trim/Pad
```fluent
trim: { s | ... },
padLeft: { s, n, c | ... },
padRight: { s, n, c | ... },
```

---

## 17. Control Flow & Iteration

### While/Until
```fluent
while: { cond, f, x | ... },
until: { cond, f, x | ... },
```

### Iterate with Index
```fluent
imap: { f, xs | xs ListMap.indexed f },  ; f receives (index, value)
```

### Find First
```fluent
find: { pred, xs | ... },
findIndex: { pred, xs | ... },
```

### Take While / Drop While
```fluent
takeWhile: { pred, xs | ... },
dropWhile: { pred, xs | ... },
```

### Span / Break
```fluent
span: { pred, xs | (takeWhile(pred, xs), dropWhile(pred, xs)) },
break: { pred, xs | span(Â¬ âˆ˜ pred, xs) },
```

### Partition
```fluent
partition: { pred, xs | (filter(pred, xs), filter(Â¬âˆ˜pred, xs)) },
```

### Group By
```fluent
groupBy: { f, xs | ... },
[1,2,3,4,5] groupBy odd  ; â†’ [[1,3,5], [2,4]]
```

### Chunks / Windows
```fluent
chunks: { n, xs | ... },         ; non-overlapping
windows: { n, xs | ... },        ; overlapping (sliding)
```

---

## 18. Geometry & Graphics

### Vector Operations
```fluent
cross3: { a, b | ... },          ; 3D cross product (dot already exists)
normalize: { v | v / â€–vâ€– },
project: { a, b | ... },         ; project a onto b
reject: { a, b | ... },          ; component perpendicular to b
reflect: { v, n | ... },
```

### Angles
```fluent
angle: { a, b | acos(dot(a,b) / (â€–aâ€– Ã— â€–bâ€–)) },
atan2: TensorAtan2,
```

### Rotation Matrices
```fluent
rot2d: { Î¸ | [[cos(Î¸), -sin(Î¸)], [sin(Î¸), cos(Î¸)]] },
rotX: { Î¸ | ... },
rotY: { Î¸ | ... },
rotZ: { Î¸ | ... },
```

### Distance Functions
```fluent
euclidean: { a, b | â€–a - bâ€– },
manhattan: { a, b | Î£(abs(a - b)) },
chebyshev: { a, b | max(abs(a - b)) },
cosine: { a, b | 1 - dot(a,b)/(â€–aâ€–Ã—â€–bâ€–) },
```

### Bounding / Clipping
```fluent
bbox: { points | (min(points), max(points)) },
clip: { lo, hi, x | lo âŒˆ (x âŒŠ hi) },
```

---

## 19. Differential Equations & Calculus

### Numerical Derivatives
```fluent
deriv: { f, x, h | (f(x+h) - f(x-h)) / (2Ã—h) },
grad: TensorGradient,            ; already have âˆ‡
jacobian: { f, x | ... },
hessian: { f, x | ... },
```

### Integration
```fluent
trapz: { x, y | ... },           ; trapezoidal rule
simpson: { x, y | ... },
romberg: { f, a, b | ... },
```

### ODE Solvers
```fluent
euler: { f, y0, t | ... },
rk4: { f, y0, t | ... },         ; Runge-Kutta 4
```

---

## 20. Neural Network Primitives

### Activations
```fluent
relu: { x | x âŒˆ 0 },
leakyRelu: { Î±, x | x âŒˆ (Î± Ã— x) },
gelu: { x | ... },
silu: { x | x Ã— sigmoid(x) },
sigmoid: { x | 1 / (1 + exp(-x)) },
softmax: TensorSoftmax,
softplus: { x | log(1 + exp(x)) },
```

### Loss Functions
```fluent
mse: { y, Å· | Î¼((y - Å·)Â²) },
mae: { y, Å· | Î¼(abs(y - Å·)) },
crossEntropy: { y, Å· | -Î£(y Ã— log(Å·)) },
binaryCrossEntropy: { y, Å· | ... },
huber: { Î´, y, Å· | ... },
```

### Layers (as higher-order functions)
```fluent
dense: { w, b | { x | x matmul w + b } },
conv2d: { kernel | { x | x âŠ› kernel } },
dropout: { p | { x | x Ã— (rand(shape(x)) > p) / (1-p) } },
batchNorm: { Î³, Î² | { x | Î³ Ã— zscore(x) + Î² } },
layerNorm: { Î³, Î² | { x | ... } },
```

### Attention
```fluent
attention: { Q, K, V | softmax(Q matmul Kâ€  / âˆšd) matmul V },
multiHead: { heads, Wq, Wk, Wv, Wo | ... },
```

---

## 21. Bit Manipulation (if integers are supported)

```fluent
(âŠ•): TensorBitXor,
(âŠ–): TensorBitOr,              ; if not used for rotate
(âŠ—): TensorBitAnd,
(â‰ª): TensorShiftLeft,
(â‰«): TensorShiftRight,
popcount: TensorPopCount,       ; count 1 bits
clz: TensorCountLeadingZeros,
ctz: TensorCountTrailingZeros,
```

---

## 22. Lazy / Infinite Sequences (if supported)

```fluent
iterate: { f, x | ... },        ; [x, f(x), f(f(x)), ...]
repeat: { x | ... },            ; [x, x, x, ...]
cycle: { xs | ... },            ; [xs..., xs..., ...]
naturals: iterate((+1), 0),
primes: { ... },
fibonacci: { ... },
```

---

## 23. Monadic Error Handling

```fluent
maybe: { default, f, x | x = null ? default : f(x) },
either: { onLeft, onRight, e | ... },
try: { f, x | ... },            ; returns (result, error)
catch: { handler, f, x | ... },
```

---

## 24. Unicode Operator Aesthetic Menu

For maximum APL aesthetic, here are beautiful Unicode operators:

| Symbol | Name | Meaning |
|--------|------|---------|
| `âŠ•` | circled plus | XOR, direct sum |
| `âŠ–` | circled minus | rotate, symmetric diff |
| `âŠ—` | circled times | outer product, tensor product |
| `âŠ˜` | circled division | ??? |
| `âŠ™` | circled dot | Hadamard product |
| `âŠš` | circled ring | ??? |
| `âŠ›` | circled asterisk | convolution |
| `âŠœ` | circled equals | ??? |
| `âŠ` | circled dash | ??? |
| `âŸ¨âŸ©` | angle brackets | vector literal |
| `âŸªâŸ«` | double angle | matrix literal |
| `âŒˆâŒ‰` | ceiling | round up |
| `âŒŠâŒ‹` | floor | round down |
| `âˆ‚` | partial | derivative |
| `âˆ«` | integral | integration |
| `âˆ¬` | double integral | ??? |
| `âˆ®` | contour integral | ??? |
| `âˆ‡` | nabla | gradient |
| `â–³` | triangle | symmetric difference, Laplacian |
| `â–¡` | box | modal necessity? |
| `â—‡` | diamond | modal possibility? |
| `â˜…` | star | special operation |
| `â˜†` | white star | conjugate? |
| `â™¯` | sharp | cardinal, count |
| `â™­` | flat | flatten |
| `â™®` | natural | ??? |
| `â€ ` | dagger | conjugate transpose |
| `â€¡` | double dagger | ??? |
| `âº` | superscript plus | positive part |
| `â»` | superscript minus | negative part, inverse |
| `â„•` | naturals | natural numbers |
| `â„¤` | integers | integers |
| `â„š` | rationals | rationals |
| `â„` | reals | real numbers |
| `â„‚` | complex | complex numbers |
| `âˆ` | infinity | infinity |
| `âˆ…` | empty set | null, empty |
| `âŠ¤` | top | true |
| `âŠ¥` | bottom | false |
| `âŠ¢` | right tack | right identity, right |
| `âŠ£` | left tack | left identity, left |
| `â«½` | double slash | parallel |
| `âŸ‚` | perpendicular | orthogonal |

---

## Summary: The Ultimate Fluent Wishlist

### Must Have (Foundation)
1. Scan `â€` â€” running reductions
2. Grade `â‹â’` â€” argsort
3. Take/Drop `â†‘â†“` â€” slicing
4. Unique `âˆª` â€” deduplication
5. All/Any `âˆ€âˆƒ` â€” boolean reduce
6. Flatten `,` â€” ravel

### Should Have (Power)
1. Outer product `âŠ—` â€” combinatorics
2. Flip `â¨` â€” argument swap
3. Fork/Hook â€” tacit programming
4. Windows/Prior â€” sliding operations
5. Group `âŠ”` â€” categorization

### Could Have (Polish)
1. Each-left/right `\:/:` â€” partial mapping
2. Under `&.` â€” conjugation
3. FFT `â„±` â€” signal processing
4. SVD/decompositions â€” linear algebra
5. Distributions â€” statistics

### Dream Features
1. Stencil `âŒº` â€” cellular automata
2. Lazy sequences â€” infinite streams
3. Pattern matching â€” destructuring
4. Macros â€” code generation
5. Dependent types â€” proof carrying code
